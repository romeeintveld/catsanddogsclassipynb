{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/romeeintveld/miniconda3/envs/PCT/lib/python2.7/site-packages (5.3.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##This notebook is built around using tensorflow as the backend for keras \n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dimensions of our images.# dimen \n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 834 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "##Used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "##Automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        classes = ['dogs', 'cats'])\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        classes = ['dogs', 'cats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile Model\n",
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "# keras.losses.binary_crossentropy\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train Model\n",
    "nb_epoch = 15\n",
    "nb_train_samples = 500\n",
    "nb_validation_samples = 832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "500/500 [==============================] - 457s 915ms/step - loss: 0.6423 - acc: 0.6281 - val_loss: 0.5977 - val_acc: 0.6760\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 472s 945ms/step - loss: 0.4615 - acc: 0.7870 - val_loss: 0.6128 - val_acc: 0.7272\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 435s 869ms/step - loss: 0.2957 - acc: 0.8754 - val_loss: 0.7766 - val_acc: 0.7659\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 421s 842ms/step - loss: 0.1599 - acc: 0.9354 - val_loss: 1.0324 - val_acc: 0.7037\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 422s 844ms/step - loss: 0.1163 - acc: 0.9572 - val_loss: 1.5421 - val_acc: 0.7240\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 421s 843ms/step - loss: 0.0870 - acc: 0.9685 - val_loss: 1.4404 - val_acc: 0.7257\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - 421s 843ms/step - loss: 0.0880 - acc: 0.9719 - val_loss: 1.9460 - val_acc: 0.7299\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - 423s 846ms/step - loss: 0.0867 - acc: 0.9741 - val_loss: 2.3954 - val_acc: 0.7407\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - 423s 846ms/step - loss: 0.0945 - acc: 0.9744 - val_loss: 1.4624 - val_acc: 0.7301\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - 419s 838ms/step - loss: 0.1066 - acc: 0.9654 - val_loss: 1.9987 - val_acc: 0.7171\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - 420s 840ms/step - loss: 0.1369 - acc: 0.9624 - val_loss: 1.9256 - val_acc: 0.7218\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - 420s 840ms/step - loss: 0.1413 - acc: 0.9635 - val_loss: 2.1815 - val_acc: 0.7276\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - 419s 838ms/step - loss: 0.1637 - acc: 0.9575 - val_loss: 1.8795 - val_acc: 0.7317\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - 416s 831ms/step - loss: 0.1958 - acc: 0.9469 - val_loss: 2.4132 - val_acc: 0.7214\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 413s 826ms/step - loss: 0.1890 - acc: 0.9543 - val_loss: 2.1508 - val_acc: 0.7294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1236730d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=nb_train_samples, \n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=nb_validation_samples, \n",
    "                    use_multiprocessing=True, \n",
    "                    shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.156786685559582, 0.729046804540507]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Evaluating on validation set\n",
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "##Data augmentation for improving the model\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "        rescale=1./255,        # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,       # randomly applies shearing transformation\n",
    "        zoom_range=0.2,        # randomly applies shearing transformation\n",
    "        horizontal_flip=True)  # randomly flip the images\n",
    "\n",
    "# same code as before\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "500/500 [==============================] - 575s 1s/step - loss: 0.7135 - acc: 0.4992 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 580s 1s/step - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - 606s 1s/step - loss: 0.6932 - acc: 0.4887 - val_loss: 0.6931 - val_acc: 0.5003\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - 569s 1s/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - 566s 1s/step - loss: 0.6932 - acc: 0.4869 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - 567s 1s/step - loss: 0.6932 - acc: 0.4914 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - 783s 2s/step - loss: 0.6932 - acc: 0.4901 - val_loss: 0.6931 - val_acc: 0.5006\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - 869s 2s/step - loss: 0.6932 - acc: 0.4927 - val_loss: 0.6932 - val_acc: 0.4995\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - 663s 1s/step - loss: 0.6932 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - 587s 1s/step - loss: 0.6932 - acc: 0.4888 - val_loss: 0.6931 - val_acc: 0.5001\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - 589s 1s/step - loss: 0.6932 - acc: 0.4928 - val_loss: 0.6931 - val_acc: 0.4998\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - 767s 2s/step - loss: 0.6932 - acc: 0.4911 - val_loss: 0.6931 - val_acc: 0.5003\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - 874s 2s/step - loss: 0.6932 - acc: 0.4881 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - 818s 2s/step - loss: 0.6932 - acc: 0.4885 - val_loss: 0.6931 - val_acc: 0.5001\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 588s 1s/step - loss: 0.6932 - acc: 0.4879 - val_loss: 0.6931 - val_acc: 0.5004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123e8f750>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator_augmented, \n",
    "                    steps_per_epoch=nb_train_samples, \n",
    "                    epochs=15,\n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=nb_validation_samples, \n",
    "                    use_multiprocessing=True, \n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931472426352727, 0.5001945979606134]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Evalutating on Validation Set\n",
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5004587]]\n"
     ]
    }
   ],
   "source": [
    "##Test model\n",
    "\n",
    "img=image.load_img('test/img001.jpg', target_size=(img_width, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "result = model.predict(x, batch_size=1)\n",
    "print result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
